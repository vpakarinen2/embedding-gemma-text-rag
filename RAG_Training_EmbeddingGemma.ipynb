{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f791095f-8e61-4e6a-9fdb-d9ec841da23b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!python -m pip uninstall -y torch torchvision torchaudio\n",
    "!python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install faiss-gpu-cu12 datasets sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c391e89d-b83a-43f0-ba8e-ebdf3b145319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"HF_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e3b7ed-4e25-44a4-a8b7-15f19a054e81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"google/embeddinggemma-300m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d86cd7f-0c1b-48c8-b9bd-311b8df4580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Dict, Sequence, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "def read_text_file(path: Path, encoding: str = \"utf-8\", max_bytes: Optional[int] = 5_000_000) -> Optional[str]:\n",
    "    try:\n",
    "        if max_bytes is not None and path.stat().st_size > max_bytes:\n",
    "            return None\n",
    "        return path.read_text(encoding=encoding, errors=\"ignore\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            return path.read_bytes().decode(\"utf-8\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def chunk_text(text: str, size: int = 1000, overlap: int = 200):\n",
    "    if size <= 0:\n",
    "        raise ValueError(\"size must be > 0\")\n",
    "    if overlap < 0:\n",
    "        raise ValueError(\"overlap must be >= 0\")\n",
    "    if overlap >= size:\n",
    "        overlap = size - 1\n",
    "    step = max(1, size - overlap)\n",
    "    for i in range(0, len(text), step):\n",
    "        chunk = text[i:i + size]\n",
    "        if not chunk:\n",
    "            break\n",
    "        yield chunk\n",
    "\n",
    "def iter_file_chunks(\n",
    "    base_dir: str | Path,\n",
    "    patterns: Sequence[str] = (\"*.txt\", \"*.md\"),\n",
    "    recursive: bool = False,\n",
    "    size: int = 1000,\n",
    "    overlap: int = 200,\n",
    "    max_bytes: Optional[int] = 5_000_000,\n",
    "    include_titles: bool = True,\n",
    ") -> Iterator[Dict[str, str | int]]:\n",
    "    base = Path(base_dir)\n",
    "    paths = []\n",
    "    for pat in patterns:\n",
    "        paths.extend(base.rglob(pat) if recursive else base.glob(pat))\n",
    "    paths = [p for p in paths if p.is_file()]\n",
    "    for p in paths:\n",
    "        text = read_text_file(p, max_bytes=max_bytes)\n",
    "        if not text:\n",
    "            continue\n",
    "        title = p.stem if include_titles else \"none\"\n",
    "        for idx, c in enumerate(chunk_text(text, size=size, overlap=overlap)):\n",
    "            yield {\"source\": str(p), \"title\": title, \"chunk\": c, \"chunk_index\": idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bedf72-b199-472c-a984-2b341b2e25bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/workspace/corpus\"\n",
    "records = list(iter_file_chunks(base_dir=base_dir))\n",
    "\n",
    "print(len(records), \"chunks\")\n",
    "\n",
    "for r in records[:3]:\n",
    "    print(r[\"source\"], r[\"chunk_index\"], len(r[\"chunk\"]))\n",
    "\n",
    "def as_doc(text, title=None):\n",
    "    return f\"title: {title if title else 'none'} | text: {text}\"\n",
    "\n",
    "doc_inputs = [as_doc(rec[\"chunk\"], rec[\"title\"]) for rec in records]\n",
    "doc_embs = model.encode_document(doc_inputs, batch_size=8, device=\"cuda\", convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7999cf-f815-4115-8a74-28d71151d990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, faiss\n",
    "\n",
    "def l2_normalize(x):\n",
    "    n = np.linalg.norm(x, axis=1, keepdims=True) + 1e-12\n",
    "    return x / n\n",
    "\n",
    "emb = l2_normalize(doc_embs.astype(np.float32))\n",
    "index = faiss.IndexFlatIP(emb.shape[1])\n",
    "index.add(emb)\n",
    "\n",
    "def retrieve(q, k=5):\n",
    "    qv = model.encode_query(q).astype(np.float32)[None, :]\n",
    "    qv = l2_normalize(qv)\n",
    "    k = min(k, index.ntotal)\n",
    "    D, I = index.search(qv, k)\n",
    "    hits = [(int(i), float(d)) for i, d in zip(I[0], D[0]) if i != -1]\n",
    "    return hits\n",
    "\n",
    "q = \"Type your query here\"\n",
    "hits = retrieve(q, k=5)\n",
    "for i, s in hits:\n",
    "    print(round(s, 4), records[i][\"title\"], records[i][\"chunk\"][:160].replace(\"\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8395da-c878-4eb3-b48f-60a137dc1b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"google/gemma-2-2b-it\"\n",
    "tok = AutoTokenizer.from_pretrained(model_id)\n",
    "gen = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "gen = gen.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7e81c7-1ed1-44c9-97aa-4b39eb452109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question, context_chunks):\n",
    "    context = \"\\n\\n\".join(context_chunks)\n",
    "    return f\"Use the context to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "topk = 3\n",
    "hits = retrieve(q, k=topk)\n",
    "ctx_chunks = [records[i][\"chunk\"] for i, _ in hits]\n",
    "prompt = build_prompt(q, ctx_chunks)\n",
    "\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(gen.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = gen.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "print(f\"Answer: {tok.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
