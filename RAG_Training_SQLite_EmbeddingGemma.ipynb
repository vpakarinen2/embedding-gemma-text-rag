{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cbddef",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!python -m pip uninstall -y torch torchvision torchaudio\n",
    "!python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install faiss-gpu-cu12 datasets sentence-transformers scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35c1252-337d-4354-8f61-19ea686328bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"HF_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43996c6-b35b-491b-a84f-452847c97da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sqlite3\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from data_sql import example_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec7a12-faa3-4663-a4e5-3e109542d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example_db(db_path='example_db.db'):\n",
    "    \"\"\"Create SQLite database and documents table.\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS documents (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            topic TEXT NOT NULL,\n",
    "            text_column TEXT NOT NULL)''')\n",
    "    cursor.execute('DELETE FROM documents')\n",
    "    cursor.executemany('INSERT INTO documents (topic, text_column) VALUES (?, ?)', example_data)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def fetch_data_from_db(db_path='example_db.db'):\n",
    "    \"\"\"Fetch all rows from the documents table.\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT id, topic, text_column FROM documents\")\n",
    "    rows = cursor.fetchall()\n",
    "    conn.close()\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3747a8cc-87a8-439f-bffc-7b20acbe355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model():\n",
    "    \"\"\"Load the embedding model (embeddinggemma-300m) with SentenceTransformer.\"\"\"\n",
    "    return SentenceTransformer(\"google/embeddinggemma-300m\")\n",
    "\n",
    "def generate_embeddings(model, texts):\n",
    "    \"\"\"Generate embeddings for a list of texts using the provided model.\"\"\"\n",
    "    return model.encode(texts, convert_to_tensor=False)\n",
    "\n",
    "def retrieve(query_embedding, doc_embeddings, top_k=3):\n",
    "    \"\"\"Retrieve top_k most similar documents to the query by cosine similarity.\"\"\"\n",
    "    sims = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "    top_indices = np.argsort(sims)[-top_k:][::-1]\n",
    "    return [(i, sims[i]) for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb645aa3-6da1-441e-bba9-0b1417559102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gemma_2b():\n",
    "    \"\"\"Load the model (gemma-2-2b-it) for causal language modeling (answer generation).\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32)\n",
    "    if torch.cuda.is_available():\n",
    "        model.to('cuda')\n",
    "    model.eval()\n",
    "    return tokenizer, model\n",
    "\n",
    "def build_prompt(question, context_chunks):\n",
    "    \"\"\"Build a prompt string that instructs to use the given context chunks to answer the question.\"\"\"\n",
    "    context = \"\\n\\n\".join(context_chunks)\n",
    "    return f\"Use the context to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "def generate_answer(tokenizer, model, question, context, max_length=256):\n",
    "    \"\"\"Generate an answer for the question based on the provided context chunks.\"\"\"\n",
    "    prompt = build_prompt(question, context)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=max_length, do_sample=True, top_p=0.9, temperature=0.7)\n",
    "    answer = tokenizer.decode(out[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f4ebf-73ad-49b6-9f90-baba78951711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Create and prepare the example database\n",
    "    create_example_db()\n",
    "    rows = fetch_data_from_db()\n",
    "\n",
    "    # Load embedding model and embed all documents\n",
    "    embed_model = load_embedding_model()\n",
    "    texts = [row[2] for row in rows]\n",
    "    doc_embeddings = generate_embeddings(embed_model, texts)\n",
    "\n",
    "    # Example user query\n",
    "    question = \"Type your query here\"\n",
    "    question_embed = generate_embeddings(embed_model, [question])[0]\n",
    "\n",
    "    # Retrieve top-3 similar documents for context\n",
    "    hits = retrieve(question_embed, doc_embeddings, top_k=3)\n",
    "    ctx_chunks = [rows[i][2] for i, _ in hits]\n",
    "    \n",
    "    # Load Gemma 2B model and tokenizer for answer generation\n",
    "    tokenizer, gen_model = load_gemma_2b()\n",
    "\n",
    "    # Generate answer based on user question\n",
    "    answer = generate_answer(tokenizer, gen_model, question, ctx_chunks)\n",
    "    \n",
    "    print(\"Answer:\", answer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
